# Teoriorienteret-metode

The R file was uploaded to github.com. The date is 1.7.23.

For text mining I use Statistical Research Consultant at University of Virginia Library Clay Ford’s tutorial “Reading PDF files into R for text mining” (link: https://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/). For testing regular expressions, I use regex101.com. The first thing I do is to download all the files from Brightspace and organize it into three different folders: “Konservative”, “Liberale og radikale” and “Socialdemokratiske og socialistiske” . In order to do it, I have to do research and read part of the files. If the author is a politician I will put him in the folder corresponding to his or her party. If the author is not a politician I will see if public records show membership of a political party, an official newspaper of a political party or similar affiliation. This procedure means that most of the texts are placed in a folder. For the rest I will read (part of) the text and by my judgement put it in the ‘right’ folder.
The next thing I do is to write a regex that tells RStudio to read PDF files (files <– list.files(pattern = “pdf$”)). In RStudio you can go to the “Sessions” in the menu bar, then go down to “Set Working Directory” and choose “Choose Directory”. Here I can choose which folder’s PDF files should be read (and in my case there are three different folders). The next step is to make the process automatic by using the lapply function which creates an object by the name of “objects” (line seven, cf. the link in item 7). Hereafter the code corp <– Corpus(URISource(files), readerControl = list(reader = readPDF)) (line 9-10) tells to create a database (the corpus) consisting of the PDF files that it should concentrate on and read. In doing so it works in principle but not de facto since we A) need a term-document matrix (TDM) counts words for each document and B) we need the regex to clean the document for useless punctuation, stopwords, numbers and such. By running the last to lines of codes (line 23-24) I can see the most used words in all the documents at once. By running the code coming before that I can see the most used words in each document if I want to compare one document with another. In both cases they are presented in a spreadsheet.
